{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-AhVYeBWgQ3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# NOTE: PLEASE MAKE SURE YOU ARE RUNNING THIS IN A PYTHON3 ENVIRONMENT\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# This is needed for the iterator over the data\n",
    "# But not necessary if you have TF 2.0 installed\n",
    "#!pip install tensorflow==2.0.0-beta0\n",
    "\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "# !pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IoM4VFxWpMR"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHQ2Ko0zl7M4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "for s,l in train_data:\n",
    "  training_sentences.append(str(s.numpy()))\n",
    "  training_labels.append(l.numpy())\n",
    "  \n",
    "for s,l in test_data:\n",
    "  testing_sentences.append(str(s.numpy()))\n",
    "  testing_labels.append(l.numpy())\n",
    "  \n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7n15yyMdmoH1"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9axf0uIXVMhO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all '\n",
      "b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NEpdhb8AxID"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                9600      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 169,997\n",
      "Trainable params: 169,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5LLrXC-uNX6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.5421 - accuracy: 0.6929 - val_loss: 0.3604 - val_accuracy: 0.8382\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.3112 - accuracy: 0.8721 - val_loss: 0.3802 - val_accuracy: 0.8307\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.2424 - accuracy: 0.9053 - val_loss: 0.3853 - val_accuracy: 0.8336\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.1934 - accuracy: 0.9283 - val_loss: 0.4146 - val_accuracy: 0.8265\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.1514 - accuracy: 0.9456 - val_loss: 0.4576 - val_accuracy: 0.8242\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.1090 - accuracy: 0.9621 - val_loss: 0.5646 - val_accuracy: 0.8175\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0764 - accuracy: 0.9746 - val_loss: 0.6202 - val_accuracy: 0.8098\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0516 - accuracy: 0.9826 - val_loss: 0.7131 - val_accuracy: 0.8145\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0364 - accuracy: 0.9879 - val_loss: 0.7970 - val_accuracy: 0.8109\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.8370 - val_accuracy: 0.8058\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.9792 - val_accuracy: 0.8177\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0168 - accuracy: 0.9940 - val_loss: 1.0570 - val_accuracy: 0.8101\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0160 - accuracy: 0.9943 - val_loss: 1.0862 - val_accuracy: 0.8080\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0130 - accuracy: 0.9952 - val_loss: 1.1453 - val_accuracy: 0.8088\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0117 - accuracy: 0.9965 - val_loss: 1.1718 - val_accuracy: 0.8104\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0076 - accuracy: 0.9977 - val_loss: 1.2542 - val_accuracy: 0.8091\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0073 - accuracy: 0.9973 - val_loss: 1.2882 - val_accuracy: 0.8094\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0096 - accuracy: 0.9968 - val_loss: 1.2480 - val_accuracy: 0.7968\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0062 - accuracy: 0.9981 - val_loss: 1.4103 - val_accuracy: 0.8102\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.0046 - accuracy: 0.9987 - val_loss: 1.2960 - val_accuracy: 0.8086\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.0075 - accuracy: 0.9977 - val_loss: 1.3045 - val_accuracy: 0.7971\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.0069 - accuracy: 0.9977 - val_loss: 1.4362 - val_accuracy: 0.8114\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.0034 - accuracy: 0.9988 - val_loss: 1.3617 - val_accuracy: 0.8071\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.0049 - accuracy: 0.9984 - val_loss: 1.4712 - val_accuracy: 0.8005\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.0052 - accuracy: 0.9985 - val_loss: 1.4956 - val_accuracy: 0.7960\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.0039 - accuracy: 0.9990 - val_loss: 1.4943 - val_accuracy: 0.8103\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.0037 - accuracy: 0.9989 - val_loss: 1.4742 - val_accuracy: 0.8013\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0045 - accuracy: 0.9983 - val_loss: 1.6364 - val_accuracy: 0.8121\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0060 - accuracy: 0.9979 - val_loss: 1.4202 - val_accuracy: 0.8048\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.0031 - accuracy: 0.9990 - val_loss: 1.5765 - val_accuracy: 0.8102\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.5449 - val_accuracy: 0.8098\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 7.8414e-05 - accuracy: 1.0000 - val_loss: 1.6257 - val_accuracy: 0.8106\n",
      "Epoch 33/50\n",
      " 3648/25000 [===>..........................] - ETA: 37s - loss: 2.9629e-05 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHGYuU4jPYaj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.style.use('seaborn')\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSualgGPPK0S"
   },
   "outputs": [],
   "source": [
    "# Model Definition with LSTM\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_Jc7cY3Qxke"
   },
   "outputs": [],
   "source": [
    "# Model Definition with Conv1D\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Course 3 - Week 3 - Lesson 2d.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
